%!TEX root = forallxbris.tex
\part{Fallacies of Reasoning}
\label{ch.Fallacies}
\addtocontents{toc}{\protect\mbox{}\protect\hrulefill\par}

\chapter{Logical fallacies}
\label{s:LogicalFallacies}


So far, we've been looking at arguments given in English, and we've been symbolising them and applying truth table tests to discover whether or not they are valid. But among the invalid ones, we haven't been distinguishing much. We said in Chapter \ref{s:Valid} that there are various invalid arguments that are nonetheless good arguments. For instance, there are inductive arguments: All Fs we've seen so far are Gs; therefore, all Fs are Gs. There are statistical arguments: x\% of Fs we've seen so far are Gs; therefore, x\% of all Fs are Gs. And there are abductive arguments: of all available hypotheses, H best explains our evidence; therefore, H is true. In this chapter, we pay a little more attention to the bad invalid arguments. Why, you might ask? Well, it's good to know your enemy. And bad invalid arguments come in many shapes and sizes, and it's helpful to be able to characterise them. This makes it easier for you to spot them in the wild. When you feel uneasy about an argument, and you show that it is not valid, you might wish to ask whether it belongs to one of the standard sorts of bad invalid argument. If it does, that will help you to combat it.


\section{Affirming the consequent}

We know that the following form of argument is valid:
\begin{earg}
\prem If \metaX, then \metaY

\prem \metaX

\conc \metaY.
\end{earg}
This form is called \emph{modus ponens}, or as we used it in our natural deduction system, $\eif$E. But here's a form of argument that you sometimes see, and it's not valid. It's called \emph{affirming the consequent}.
\begin{earg}
\prem If \metaX, then \metaY

\prem \metaY

\conc \metaX.
\end{earg}
Here's an example:
\begin{earg}
\prem If the universe were created by a benevolent God, people would behave altruistically, at least some of the time.

\prem People do behave altruistically, at least some of the time.

\conc The universe were created by a benevolent God.
\end{earg}
This is invalid because it's possible for the premises to be true and the conclusion false. The reason is that the first premise tells us one possible reason why people would behave altruistically, namely, that they are part of a universe created by a benevolent God. But it doesn't rule out that there are other possible reasons. It might be that altruistic behaviour has evolved by natural selection because altruistic groups are better able to cooperate and thus increase their survival chances and their evolutionary fitness. 


\section{Quantifier shift fallacy}

Here is an argument that is sometimes ascribed to Aristotle (though it isn't at all clear this is what Aristotle actually meant):
\begin{earg}
\prem Every agent acts for an end.

\conc Therefore, there is some (one) end for which every agent acts.
\end{earg}
And here's an argument from St Thomas Aquinas's Third Way (the third among his five ways to argue for the existence of God):
\begin{earg}
\prem Everything at some time fails to exist.

\conc There must be some (one) time at which everything fails to exist.
\end{earg}
In both cases, the same thing goes wrong. The first premise says that, for every thing, there is some other thing related to the first thing in a particular way. The conclusion says that there is some one thing that is related to everything in that way. So Aristotle is said to argue that, since for every agent there is some end for which that agent acts, there must be a single end for which all agents act. And Aquinas is said to argue that, since for every thing, there is some time at which it does not exist, there must be some time at which everything does not exist. In both cases, they are committing a quantifier shift fallacy. It's perfectly possible that each agent has a different end for which they act. That would make Aristotle's premise true and his conclusion false. It's perfectly possible that each thing exists for four years, say, and at any time, some thing exists. That would make Aquinas' premise true and his conclusion false.

\section{Ad hominem fallacy}

The name comes from Latin, where `ad hominem' means `to the person'. This form of argument concludes against a claim by attacking the person or group of people who put forward the claim, even when the attack is quite irrelevant to the truth of the claim in question. It is very obviously fallacious, but that doesn't stop it from being rather widespread in philosophy! Here's a rather unsubtle example:
\begin{earg}
\prem The original proponent of the theory of natural selection was Charles Darwin.

\prem Charles Darwin ate meat.

\prem Eating meat is wrong.

\conc The theory of natural selection is wrong.
\end{earg}
This is unsubtle because Darwin's morality is surely irrelevant to the correctness of his theory of evolution. But there are other cases where things aren't quite so clear cut. For instance, suppose you criticise the actions of your government. You then learn that a virulently racist hate group has also criticised the government for those same actions. Is it a good objection to your criticism to point out that evil people hold the same view? The American philosopher Joshua Blanchard calls this \emph{the problem of unwelcome epistemic company}.

It is surely not a good argument just to point out that someone evil agrees with you. After all, presumably all evil people in history agree with me that the sky is blue! But perhaps the argument has more force if the conclusion on which they agree with me is a moral one? Still, however, it doesn't seem to be a good argument. Here is Noam Chomsky explaining why:
\begin{quote}
``[I]t doesn’t bother me if I happen to agree with the mainstream media [in criticizing the  Soviet  Union]. Trotsky [\ldots] was charged in the 1930s with agreeing with the fascists in his condemnation of the Soviet Union. And he pointed out that his critique was true; he wasn't going to abandon it if somebody else happened to say it for different reasons.'' 

(Chomsky, Q\&A after `Manufacturing Consent: The Political Economy of the Mass Media', The University of Wisconsin---Madison, March 15th, 1989 (\url{https://chomsky.info/19890315/}))
\end{quote}
And Chomsky is right. The point is that you might think something for one reason, while the evil person thinks it for another reason. For instance, you and an evil person might agree that there should be no death penalty for some particularly horrible crime. You might think it because you think there should be no death penalty for any crime at all; they might think it because they think that the horrible crime should not in fact be a crime at all. 

Nonetheless, unwelcome epistemic company should give us pause. It should force us to think about what our reasons are for our conclusions, and what the evil person's reasons are, so that we can ensure our reasons are good.

\section{Begging the question}

Sometimes this is called `petitio principii'. Both are terrible names for it, but sadly we're stuck with them. The original name in the Greek means `assuming the conclusion'. This was then mistranslated into Latin as `petitio principii', and this was mistranslated again into English as `begging the question'. Nowadays, people who haven't studied philosophy often use `beg the question' to mean `raise the question'. Philosophers sometime get very upset about this, which is a bit silly, given the history of the term in philosophy. In any case, in the philosophers' sense, you beg the question when you argue for a conclusion, but you sneak that very conclusion itself into one of your premises. As a result, your argument actually ends up being valid, but it won't convince anyone who didn't already believe the conclusion.

This is a particular problem in philosophy, since often we are not giving arguments for a conclusion in order to convince people of that conclusion --- often we all agree with the conclusion, but we're trying to figure out how you might justify it rather than simply take it for granted. For instance, suppose we try to answer the sceptic who says that the external world doesn't really exist (maybe it's Descartes talking about his evil demon in the \emph{Meditations}). So we give an argument for the existence of the external world. The conclusion of our argument is that the external world exists. We don't expect that there's any real person who needs this argument to convince them that it exists. Rather, we give it to show that our core beliefs are justified and not just accepted on faith or because we're told they're true when we're young. 

A good example of begging the question is G. E. Moore's famous argument against the sceptic. Moore held out his hands and said:
\begin{earg}
\prem Here is one hand.

\prem And here is another.

\conc There are at least two external objects in the world.
\end{earg}
But of course the sceptic won't accept the first or second premise here. Moore has assumed his conclusion in his premises.

%\section{False dilemma (or false dichotomy)}

%This fallacy is often found in political discussion. In it, we assume that there are only two options: either A or B. We then argue against A and conclude that B. Now, formally, that This implicitly assumes that
%these are the only two options.
%we can’t stand by and do nothing, therefore we have to
%carry out my preferred policy
%We can make this a valid argument by making the implicit
%premise A ∨B (“my way or no way”) explicit. But this valid argument has a premise that we don’t want to accept, so doesn’t lead
%us to accepting the conclusion.
%⊲ Irrelevant conclusion.
%The prosecution will show that the defendant in the
%dock was indeed the murderer. The crime was heinous,
%committed by a callous killer who is a danger to society,
%and such a crime cannot be allowed to go unpunished.
%This is an argument of the form A ∴ B. The premise and conclusion are unconnected and the premise provides no support for
%the conclusion.

\practiceproblems

\problempart Give your own example of each of the following fallacies:
\begin{enumerate}
\item Affirming the consequent
\item Quantifier shift fallacy
\item Ad hominem fallacy
\item Begging the question
\end{enumerate}

\problempart Identify which fallacy the following arguments commit:
\begin{enumerate}
\item John Locke thought that the criterion for personal identity is psychological continuity. But he wrote in support of slavery in \emph{The Fundamental Constitutions of Carolina}. Therefore, personal identity cannot consist in psychological continuity.
\item Cigarettes are deadly, since cigarettes will kill you. 
\item When it's about to rain, cows go under nearby trees. And indeed they are going under the trees. So it must be about to rain.
\item Everything that happens must have a cause, and so there must be some first thing that causes everything else.\footnote{This is sometimes (mis)attributed to the early modern British empiricist philosopher, John Locke.}
\end{enumerate}


\chapter{Probabilistic fallacies}
\label{s:ProbFallacies}



Let's now move on to a couple of probabilistic fallacies. These are arguments that appeal to probabilities in some way, but they go badly wrong by misunderstanding how probabilities work. While you encounter this sort of reasoning less often in philosophy texts, they come up all the time in everyday life. Misuse of probabilities is very widespread and can lead to disastrous decision-making.

\section{Base rate fallacy}

This is perhaps the most famous probabilistic fallacy. It's famous partly because two social psychologists called Amos Tversky and Daniel Kahneman surveyed a group of students at  Harvard Medical School in the 1970s, and a large proportion of them failed to identify it as a fallacy! Given that the argument, at least in the form presented to them and presented here, concerns the chance of having a disease, that is definitely cause for concern!

Our example of the base rate fallacy needs a bit of a backstory. Here goes:
\begin{quote}
\textbf{Disease X } A new test is developed for disease X. It has a sensitivity of 99\% and a specificity of 99\%. That means that, if you have the disease, your chance of testing positive is 99\%, and if you don't have the disease your chance of testing negative is 99\%. The incidence of the disease in the population is 0.01\%. That is, 1 in 10,000 people have it. You happen to be passing a testing station and you take a test. It comes back positive. What is the chance that you have disease X? 
\end{quote}
Many people (including lots of those Harvard medical students) answer 99\%. A good deal more give an answer above 10\%. The correct answer is around 1\%! So anyone who argued for one of the higher answers has committed a fallacy of probabilistic reasoning.

Why is the answer around 1\%? The easiest way to see why is to ask how these probabilities would play out in a population of 1,000,000 people. The incidence of the disease is 0.01\% (i.e. 1 in 10,000), so 100 people in this population have the disease, while 999,900 do not. Now suppose that they are all tested. Of the 100 who have the disease, 99\% will test positive and 1\% will test negative. So 99 will test positive and 1 will test negative. Of the 999,900 who do not have the disease, 99\% will test negative and 1\% will test positive. So 989,901 will test negative, and 9,999 will test positive. So, 99 + 9,999 = 10,098 will test positive in total. Of those, 9,999 will not have the disease, and 99 will have it. 99 is around 1\% of 10,098. So, if you test positive, you've a 1\% chance of being someone with the disease who got a true positive test, and 99\% chance of being someone without the disease who got a false positive test. 

It is called the base rate fallacy because people who commit it ignore how uncommon the disease is in the population---that's the base rate of the disease in the population. Because the disease is so uncommon, you're very unlikely to have it. And so if you get a positive test it's much more likely that it's a false positive and you don't have the disease than it is that it's a true positive and you do have the disease. This is one reason why doctors are so worried about the specificity and sensitivity of their tests. 

\section{Simpson's Paradox}

This fallacy arises when people see a trend in two separate samples and infer that the same trend will occur when the samples are merged. Let's see what this means in a particular example. Suppose I have kidney stones. I go to my doctor and I ask what sort of treatment she'd recommend. She tells me that there are two available: Treatment 1 and Treatment 2. She also tells me that doctors divide kidney stones into two types: small ones and large ones. She tells me that, when treating small kidney stones, Treatment 1 has been successful a greater proportion of the time it's been used than Treatment 2. And she also tells me that, when treating large kidney stones, Treatment 1 has been successful a greater proportion of the time it's been used than Treatment 2. `OK,' I say, `sounds like a no-brainer---I should go for Treatment 1, because it has been more successful a greater proportion of the time it's been used than Treatment 2.' Unfortunately, though, this reasoning is bad.

It's easy to see why it's tempting, but it's nonetheless bad probabilistic reasoning. It's tempting because it looks a lot like a valid form of reasoning in truth-functional logic: 
\begin{earg}
\prem \metaX or \metaY.
\prem If \metaX, then \metaZ.

\prem If \metaY, then \metaZ.

\conc Therefore, \metaZ.
\end{earg}
This is basically what we used in the natural deduction rule $\eor$E. It is a valid argument form (check with truth-tables!). And my reasoning about the two treatments seems very close to this. I reason:
\begin{earg}
\prem For small kidney stones, Treatment 1 has been successful a greater proportion of the time it's been used than Treatment 2.

\prem For large kidney stones, Treatment 1 has been successful a greater proportion of the time it's been used than Treatment 2.

\prem Kidney stones are either small or large.

\conc For kidney stones in general, Treatment 1 has been successful a greater proportion of the time it's been used than Treatment 2.
\end{earg}
But this reasoning is bad. How do we show that? Well, remember that, to show that an argument is invalid in truth-functional logic, we describe a case in which the premises are true and the conclusion false. Here, we give a case in which the probabilities are as first and second premise say they are, and the third premise is true, but the probabilities are not as the conclusion says they are. Here's the case: suppose the following table gives the success rates of each treatment for each size of kidney stone.
\begin{center}
\begin{tabular}{r|r|r}
& Treatment 1 & Treatment 2 \\
\hline 
Small & 9 out of 10 (90\%) & 80 out of 100 (80\%) \\
Large & 70 out of 100 (70\%) & 6 out of 10 (60\%) \\
\hline 
Total & 79 out of 110 (72\%) & 86 out of 110 (78\%) 
\end{tabular}
\end{center}
So, Treatment 1 has a higher success rate than Treatment 2 in treating small kidney stones, and a higher success rate in treating large kidney stones; but it has a lower success rate in treating kidney stones! Such cases are known as instances of Simpson's paradox. It's paradoxical, because intuitively it doesn't seem like these cases should be possible. But they are!

\section{How do probabilities work?}

We said above that the base rate fallacy and Simpson's Paradox arise because people misunderstand how probabilities work. So how do they work? This is a huge question and the subject matter of all of statistics. But we can introduce the fundamental ideas here. This can end up looking a bit mathsy, but if you're not terribly comfortable with maths, it's still very worthwhile to work through. It's very useful to be able to understand how probabilities work and to be able to identify when reasoning with them goes wrong. People sometimes say `You can prove anything with statistics'. That is really very false. But people do often make bad probabilistic or statistical arguments for their conclusions, and knowing how probabilities work really helps you to spot when they're doing that.

Let's start a little abstractly, and then we'll move on to real examples. So suppose you have two atomic sentences, $A$ and $B$. As we know, there are four possible valuations. That is, there are four ways to assign truth values to these atomic sentences. Here they are, displayed as the first two columns of a truth table:
\begin{center}
\begin{tabular}{cc}
$A$ & $B$ \\
\hline
T & T \\
T & F \\
F & T \\
F & F
\end{tabular}
\end{center}
Each valuation represents a possible ``case''. The first row represents the worlds where $A$ and $B$ are both true, for instance. When we assign probabilities, we assign numbers to each valuation. And we make sure of two things:
\begin{itemize}
\item first, each number is either 0 or 1 or something in between them---e.g. 0.1, or 0.7, or 0.1987362;
\item second, when we add up the numbers we assign to all the valuations, they add up to 1.
\end{itemize}
So, for instance, here are four assignments of probabilities ($p_1$, $p_2$, $p_3$, $p_4$) to the valuations over $A$ and $B$. Notice that, in each case, the assignments  add up to 1; that is, the numbers in each column add up to 1.
\begin{center}
\begin{tabular}{cc|c|c|c|c}
$A$ & $B$ & $p_1$ & $p_2$ & $p_3$ & $p_4$\\
\hline
T & T & 0.1 & 0.5 & 0.25 & 0.9\\
T & F & 0.3 & 0.2 & 0.25 & 0 \\
F & T & 0.2 & 0.1 & 0.25 & 0 \\
F & F & 0.4 & 0.2 & 0.25 & 0.1 
\end{tabular}
\end{center}
OK, so that gives our assignments of probabilities to the valuations. But now we want to know the probabilities of the atomic sentences, and indeed the probability of any sentence built up from $A$ and $B$ using truth-functional connectives. How do we get that?

To get the probability of a sentence $X$, we look at the probabilities assigned to each of the valuations that make it true, and we add them up. So, for instance, the probability of $A$ according to probability assignment $p_1$ is $p_1(A) = 0.1 + 0.3 = 0.4$, since $A$ is true at the first two valuations, and $p_1$ gives them probability 0.1 and 0.3 respectively. Similarly, the probability of $B$ on $p_1$ above is $p_1(B) = 0.1 + 0.2 = 0.3$.

What about the probability of $A \eor B$ and the probability of $A\eand B$? Well, let's look at the truth tables:
\begin{center}
\begin{tabular}{cc|c|c|c|c|c|c}
$A$ & $B$ & $p_1$ & $p_2$ & $p_3$ & $p_4$ & $A \eor B$ & $A\eand B$ \\
\hline
T & T &  0.1 & 0.5 & 0.25 & 0.9& T & T \\
T & F  & 0.3 & 0.2 & 0.25 & 0 & T & F\\
F & T  & 0.2 & 0.1 & 0.25 & 0 & T & F\\
F & F  & 0.4 & 0.3 & 0.25 & 0.1 & F & F
\end{tabular}
\end{center}
So, for $p_1$, the probability of $A\eand B$ is $p_1(A\eand B) = 0.1$, since $A\eand B$ is true only on the first valuation, and the probability that $p_1$ assigns to that is $0.1$. And, for $p_1$, the probability of $A \eor B$ is $p_1(A \eor B) = 0.6$, since $A \eor B$ is true on the first three valuations, and $p_1$ assigns probabilities $0.1$, $0.3$, and $0.2$ to them, and $0.1 + 0.3 + 0.2 = 0.6$.

\subsection{General properties}
We can also see some general things about probabilities of sentences which are logically related:
\begin{itemize}
\item For any sentence $X$, $p(\enot X) = 1 - p(X)$.

That is, you get the probability of the negation of a sentence by subtracting the probability of the sentence from 1. Why? Well, the probability of $X$ is the sum of the probabilities of the valuations at which $X$ is true, while the probability of $\enot X$ is the sum of the probabilities of the valuations at which $X$ is false. Since the probabilities assigned to all the valuations must add up to 1, we get $p(X) + p(\enot X) = 1$, and so $p(\enot X) = 1 - p(X)$.
\item For any sentences $X$, $p(X \eor Y) = p(X) + p(Y) - p(X\eand Y)$. 

It's worth thinking this through. The probability of $X \eor Y$ is the sum of the probabilities of the valuations at which $X \eor Y$ is true; that is, the valuations at which $X$ or $Y$ or both are true. Now, first, add up the probabilities of the valuations at which $X$ is true. That gives $p(X)$. Second, add up the probabilities of the valuations at which $Y$ is true. That gives $p(Y)$. But at this point we've double counted those valuations at which $X$ and $Y$ are both true. We counted them once when we calculated $p(X)$ and we counted them again when we calculated $p(Y)$. So we need to subtract one lot of them. But of course the valuations at which $X$ and $Y$ are both true are exactly the valuations at which $X\eand Y$ is true. So we subtract $p(X\eand Y)$ from $p(X) + p(Y)$. So $p(X \eor Y) = p(X) + p(Y) - p(X\eand Y)$.
\end{itemize}


\subsection{Conditional Probabilities}
So that gives us the probabilities of different sentences. There is one further probabilistic notion that is very important. It's the notion of a conditional probability. Because often we don't just ask how likely $X$ is; we ask how likely $X$ is \emph{given that $Y$ is true}. For instance, I ask how likely I am to have the disease \emph{given I tested positive}. Or I ask how likely it is that we'll play rounders on the Downs \emph{given that it is raining}. So, if we have two sentences, $X$ and $Y$, and a probability assignment $p$, we write $p(X | Y)$ for the probability of $X$ given $Y$. Now, how do we calculate $p(X |Y)$? Well, here's the definition:
\[
p(X | Y) = \frac{p(X\eand Y)}{p(Y)}
\]
providing $p(Y)$ is not 0. Let's try to unpack that. Here's one way to see what's happening: the probability of $X$ given $Y$ is the proportion of the probability of $Y$ that is assigned to valuations that make $X$ true as well. The probability assigned to $Y$ is obviously $p(Y)$. And the probability assigned to valuations that make $X$ true as well as $Y$ is just the probability of $X \eand Y$, which is $p(X \eand Y)$. So the proportion of the probability of $Y$ that is assigned to valuations that make $X$ true as well is just $\frac{p(X\eand Y)}{p(Y)}$, which is how the definition defines $p(X | Y)$.

So let's see it in action. Sticking with probability assignment $p_1$, what's the probability of $A$ given $B$?
\[
p_1(A | B) = \frac{p_1(A\eand B)}{p_1(B)} = \frac{0.1}{0.3} = \frac{1}{3} = 0.333\ldots
\]
The proportion of the probability of $B$ that is assigned to valuations at which $A$ is also true is one-third.

Finally, we can state what is perhaps the most important fact in the theory of probability. It's called Bayes' Theorem, and it was first stated by the Reverend Thomas Bayes, an eighteenth century Presbyterian clergyman who lived in Kent. It's beautifully simple, but incredibly powerful:\footnote{The proof is quite short, but don't worry if you don't follow it. Here it goes. By definition 
\[
p(A) p(B|A) = p(A\eand B) \mbox{\ \ \ and\ \ \ } p(\enot A)(B | \enot A) = p(\enot A\eand B)
\]
So
\[
\frac{p(A) \times p(B|A)}{(p(A) \times p(B|A)) + (p(\enot A)\times p(B|\enot A))} = \frac{p(A\eand B)}{p(A\eand B) + p(\enot A\eand B)}
\]
But
\[
p(A\eand B) + p(\enot A\eand B) = p(B)
\]
So
\[
\frac{p(A\eand B)}{p(A\eand B) + p(\enot A\eand B)} = \frac{p(A\eand B)}{p(B)} = p(A|B)
\]
}
\[
p(A | B) = \frac{p(A) \times p(B|A)}{(p(A) \times p(B|A)) + (p(\enot A)\times p(B|\enot A))}
\]
So this allows you to calculate $p(A|B)$ if you know $p(A)$ (and therefore you also know $p(\enot A)$), $p(B|A)$, and $p(B | \enot A)$. And it turns out that often we do know these latter three values. In fact, let's see all of this in action in the base rate fallacy, where Bayes' Theorem plays a big role.

Let $A$ stand for \emph{I have the disease}. Let $B$ stand for \emph{I test positive}. Then, the way the example is set out, we're told some information about the probability assignment:
\begin{itemize}
\item We're told that the incidence of the disease in the population is 1 in 10,000. So $p(A) = 0.0001$.
\item We're told that the sensitivity and specificity of the disease is 99\%. So $p(B | A) = 0.99$ and $p(\enot B | \enot A) = 0.99$, so $p(B | \enot A) = 0.01$.
\end{itemize}
This gives us enough to use Bayes' Theorem to calculate the probability I have the disease given that I test positive, i.e., $p(A | B)$. According to Bayes' Theorem:
\begin{multline*}
p(A | B) = \frac{p(A) \times p(B|A)}{(p(A) \times p(B|A)) + (p(\enot A)\times p(B|\enot A))} = \\ \frac{0.0001 \times 0.99}{0.0001 \times 0.99 + 0.9999 \times 0.01} \approx 0.0098
\end{multline*}
And this is what we calculated above as well.

\practiceproblems

\problempart Look at probability assignment $p_2$ above. Calculate the following probabilites.
\begin{enumerate}
\item $p_2(A)$
\item $p_2(B)$
\item $p_2(A\eand  B)$
\item $p_2(A \eor B)$
\item $p_2(B\eif A)$
\item $p_2(A | B)$
\item $p_2(B | A)$
\end{enumerate}
If you feel you still need more practice, try doing the same for $p_3$ and $p_4$.



\problempart Suppose I give you the following probabilities:
\[
p(A) = 0.3 \qquad p(B | A) = 0.6 \qquad p(B | \enot A) = 0.4
\]
Use them and Bayes' Theorem to calculate $p(A | B)$.

\problempart Go back to the Base Rate Fallacy and show using Bayes' Theorem that the answer is $1\%$. 

\problempart Explain why the following facts hold for any sentences $X$, $Y$.
\begin{enumerate}
\item $p(X) \leq p(X \eor Y)$
\item $p(X\eand Y) \leq p(X)$
\item If there is no valuation where $X$ and $Y$ are both true, $p(X \eor Y) = p(X) + p(Y)$.
\end{enumerate}

